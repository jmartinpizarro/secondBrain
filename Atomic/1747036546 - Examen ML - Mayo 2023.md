---
aliases:
  - Examen ML - Mayo 2023
tags:
"References":
cssclasses:
---
# Examen ML - Mayo 2023

**Describir el algoritmo K-Means conceptualmente, sin usar ecuaciones.**

*K-Means* es un algoritmo de aprendizaje no supervisado que se basa en agrupar las instancias en clusters (el número lo define el desarrollador) usando centroides.

El algoritmo se basa en:

1. **Inicializar**: situar aleatoriamente los centros de los centroides de las agrupaciones.
2. **Repetir hasta que haya una convergencia (los centroides ya no se mueven casi nada):**
	1. **Asignación**: a cada instancia se la asocia un cluster determinado, usualmente el más cercano al que se encuentre
	2. **Actualización**: se actualizan los centros de gravedad, volviendo más preciso el centro del centroide ya que depende de las instancias asignadas.

---

**Explicar, de manera conceptual, las diferencias principales de los tres métodos de aprendizaje por refuerzo: iteración de valor (programación dinámica), método de MonteCarlo y Q-Learning**.

La principal diferencia entre el método de MonteCarlo, Q-Learning e iteración de valor es que los dos primeros usan una técnica de *exploración y explotación*, donde aprender a partir de los sucesos pasados. 

Sin embargo, iteración de valor necesita de manera implícita la tabla de la distribución de las probabilidades tal que aplicando la acción $a$ en un estado $S_t$ se llegue a un determinado estado $S_{t+1}$.

La principal diferencia entre el método de MonteCarlo y Q-Learning es que el primero **no puede actualizar la tabla Q mientras el episodio sigue en proceso**, Q-Learning sí que puede.

---

**Sean estos dos métodos: Bagging usando árboles como modelos base; y Random Forests. Explicar similitudes y diferencias entre ellos**

Es importante destacar que tanto BAGGing como Random Forests son *ensembles* cuyos modelos base son árboles. Ambos construyen cada árbol usando [[1747037274 - Muestreo por reemplazo|Muestreo por reemplazo]] a partir del dataset de entrenamiento.

Sin embargo, RF tiene un segundo mecanismo para generar aleatoriedad: la randomización. Esta consiste en generar el mejor atributo para cada árbol (usando la entropía o Gini). Bagging, por otro lado, solamente elige el mejor de todos los atributos; mientras que RF elige el mejor de un subconjunto aleatorio de atributos $mtry$. Este es uno de los hiperparámetros más importantes de RF.

---

**Describir la característica principal que diferencia los problemas de aprendizaje automático  
supervisados y no-supervisados**

En los problemas supervisados, la variable de respuesta $Y$ está disponible en nuestro dataset mientras que en los no supervisados no existe esa opción.

En los problemas supervisados tendemos a realizar modelos basados en regresión, clasificación, basados en árboles, en gradientes, máquinas vectoriales... para relacionar las variables de entrada con la variable de salida. Por otro lado, se puede hacer algo parecido en los problemas no supervisados (calcular la $Y$) usando métodos de agrupamiento como clustering, DB-Scan o jerarquización. 

---

**Supongamos que tenemos un conjunto de datos disponibles D y queremos construir y  
evaluar un modelo usando sólo los atributos más relevantes. Supongamos que seguimos el  
siguiente proceso: 1) usamos un método de selección de atributos usando el conjunto D y  
elegimos los mejores; 2) después dividimos D en una partición de entrenamiento y otra de  
test; 3) construimos el modelo con la de entrenamiento y lo evaluamos con la de test. ¿Es  
correcto este proceso? ¿Por qué?**

El proceso es incorrecto. Eso es porque se realiza la EDA antes de dividir los datos de entrenamiento y test, pudiendo filtrar cierta información al modelo de manera inherente y reduciendo su capacidad de generalizar.

Para solucionar, primero se debe de realizar el *split* del dataset $D$ para luego realizar una EDA. De esta manera, no hay un *data leakage* en los datos y el modelo no tendrá *overfitting* (que puede estar causado por esto).