---
aliases:
  - Regresi칩n Lineal
tags:
"References":
cssclasses:
---
# Regresi칩n Lineal

El objetivo es ajustar la curva (modelo) para poder generalizar los datos de manera correcta.

![[Pasted image 20250330193058.png]]

Existe siempre una variable $Y$ a predecir y una serie de variables independientes $x$. 

Existen dos maneras:
- Anal칤tica
- Iterativa, mediante descenso de gradiente
## Modelos Lineales de soluci칩n anal칤tica

Es muy costosa si tiene muchos atributos. Tiene una complejidad de $O(M^3)$.

Existe la posibilidad que, **si los atributos est치n muy correlacionados**, la matriz ($X^T \cdot X$) no sea invertible o muy inestable (en su defecto se puede usar la SVD).

Por ello, existe la **mejora iterativa mediante descenso de gradiente**.

## Modelos Lineales mediante descenso de gradiente

Para los modelos lineales, el error tiene forma de par치bola, por lo que existe **un 칰nico punto donde el error es m칤nimo**. La idea es obtener un punto lo m치s cercano a dicho punto.

![[Pasted image 20250330193714.png]]

1. Inicializar ($w_0, w$) aleatoriamente tal que:
$$y(x, w, w_0) = w_0 + w_1 \cdot x_1 + ... + w_M \cdot x_M$$
2. Durante un determinado n칰mero de iteraciones (epochs):
$$J = \frac{1}{2N} \cdot \sum_{i=1}^{i=N}(t_i - y(x,w,w_0))^2$$
$$w_0 \leftarrow w_0 - \alpha \frac{\delta J}{\delta w_0}$$
$$...$$

### Variante estoc치stica

Se actualizan los par치metros de cada instancia. 

Suele ser m치s r치pida, porque se producen m치s actualizaciones de pesos que con la versi칩n batch. Cuando se usa con redes de neuronas, funciona mejor para evitar m칤nimos locales.

## Regresi칩n Polin칩mica

Para modelar datos no lineales usando regresi칩n lineal.

Basta con a침adir tablas de datos $X^2, ..., X^P$, donde 
$$P = \text{grado del polinomio, act칰a como hiperpar치metro que controla la complejidad del modelo}$$

## Regularizaci칩n para overfitting

Incluso modelos simples como estos pueden tener overfitting si tienen demasiados pesos. 

>[!DANGER]
>Si un modelo tiene 1 variable relevante, y 99 irrelevantes, es posible que el modelo tome como referencia ciertas variables irrelevantes por culpa de los pesos $w$. 

### Regularizaci칩n Ridge o L2

Se a침ada para optimizar el modelo, un t칠rmino para penalizar la complejidad del modelo y forzar a que los $w$ sean peque침os.

Se basa en la suma de los coeficientes al cuadrado.

De esta manera, un coeficiente $w_j$ podr치 tener un valor alto, s칩lo si contribuye a reducir el error. De otra manera, su penalizaci칩n $洧녻_洧녱$ ser치 demasiado grande, y la optimizaci칩n tender치 a disminuir su valor, y como consecuencia, que la variable $x_j$ tenga menos relevancia para el modelo.

### Regularizaci칩n Lasso o L1

Con Ridge los coeficientes disminuyen, pero nunca llegan a 0. Para conseguir que los pesos $w_j$ lleguen a 0, se usa Lasso.

Se basa en el error absoluto, no en la suma de coeficientes al cuadrado.

Somos capaces de descartar variables en su totalidad.

### Elasticnet

Combinaci칩n de L1 y L2. Se parametriza con $\alpha$. Dependiendo del valor de $\alpha \in [0, 1]$, se tira m치s de L1 o de L2 respectivamente. 

## Regresi칩n log칤stica

Usado para problemas de clasificaci칩n. 
Se usa un modelo lineal, cuya salida se le pasa a la funci칩n sigmoide. La funci칩n log칤stica es la aplicaci칩n particular de la funci칩n lineal de los datos a un funcional:
$$\sigma(z) = \frac{1}{1+e^{-z}}$$
Es una funci칩n suave y progresiva, que representa la posibilidad que un instancia pertenezca a una clase u a otra.

>[!NOTE]
>Las redes de neuronas est치n basadas en un modelo de regresi칩n log칤stica anidado.

